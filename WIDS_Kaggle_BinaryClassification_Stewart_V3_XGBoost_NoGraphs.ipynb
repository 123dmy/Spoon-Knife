{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification of Survival Rates from Emergency Room Visits\n",
    "\n",
    "#### XGBoost, no graphs\n",
    "\n",
    "## Rebecca Stewart\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The challenge is to create a model that uses data from the first 24 hours of intensive care to predict patient survival. MIT's GOSSIS community initiative, with privacy certification from the Harvard Privacy Lab, has provided a dataset of more than 130,000 hospital Intensive Care Unit (ICU) visits from patients, spanning a one-year timeframe. This data is part of a growing global effort and consortium spanning Argentina, Australia, New Zealand, Sri Lanka, Brazil, and more than 200 hospitals in the United States.\n",
    "\n",
    "Labeled training data are provided for model development; you will then upload your predictions for unlabeled data to Kaggle and these predictions will be used to determine the public leaderboard rankings, as well as the final winners of the competition.\n",
    "\n",
    "Data analysis can be completed using your preferred tools. Tutorials, sample code, and other resources will be posted throughout the competition at widsconference.org/datathon and on the Kaggle Discussion Forum. The winners will be determined by the leaderboard on the Kaggle platform at the time the contest closes February 24."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\rebec\\Anaconda3\\envs\\uwdatasci420\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import gc\n",
    "import string\n",
    "import time\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import scipy.stats as ss\n",
    "from scipy import stats\n",
    "import lightgbm as lgb \n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_records=5000\n",
    "\n",
    "\n",
    "#pip install xgboost\n",
    "#setting display options\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('max_colwidth', 500)\n",
    "np.set_printoptions(linewidth =400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions \n",
    "The first one, reduce_mem_useage, was taken from Kaggle.  Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "def predict_evaluate(X_test, y_test, classifier):\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    yhat = classifier.predict_proba(X_test)\n",
    "    pos_probs = yhat[:, 1]\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    acc_score=accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy Score \" + str(acc_score))\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pos_probs)\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_test, pos_probs)\n",
    "    plot_roc_curve(fpr, tpr, roc_auc)\n",
    "    print(\"AUC \" + str(roc_auc))\n",
    "    \n",
    "def plot_roc_curve(fpr, tpr, auc):\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %.3f)'%auc)\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Load the Dataset</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91713, 186)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dataset  -- location is as follows\n",
    "#https://www.kaggle.com/c/widsdatathon2020/download/RJVMMbUOhozhdYOtAUtz%2Fversions%2FwogAJBiaORb3PdRnSB0M%2Ffiles%2Fsamplesubmission.csv\n",
    "# downloaded on 1/13/2020 at 9:20 am\n",
    "\n",
    "load_unlabled_data = False\n",
    "limit_dataset_size = True\n",
    "\n",
    "## Read the .csv file with the pandas read_csv method an create a dataframe\n",
    "tr_data = pd.read_csv(\"training_v2.csv\")\n",
    "\n",
    "if load_unlabled_data: ul_data = pd.read_csv(\"unlabeled.csv\")\n",
    "\n",
    "info_df = pd.read_csv('WiDS Datathon 2020 Dictionary.csv')\n",
    "tr_data.head(5)\n",
    "\n",
    "if limit_dataset_size: \n",
    "    tr_data = tr_data.head(max_records)\n",
    "    # tr_data = resample(tr_data, replace=False, n_samples=max_records) \n",
    "    \n",
    "tr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "tr_data.drop(['encounter_id', 'patient_id'], axis=1, inplace=True)\n",
    "tr_data.drop(['readmission_status'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer / Binary Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before converting to int or bool, we need to figure out what to do with missing values.\n",
    "\n",
    "#### Drop features with a large number of missing values\n",
    "\n",
    "Let's take a look at those columns that have more than 80% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe of the columns that have more than a certain percentage missing values\n",
    "pct_cutt_off =  .8\n",
    "isna_counts = tr_data.isna().sum()\n",
    "isna_counts = pd.DataFrame({'col':isna_counts.index, 'cnt':isna_counts.values})\n",
    "isna_counts[\"pct\"] = isna_counts['cnt']/tr_data.shape[0]\n",
    "isna_counts = isna_counts[isna_counts.loc[:, \"pct\"] > pct_cutt_off] \n",
    "isna_counts.sort_values(by=['col']).head(100)\n",
    "drop_cols = list(isna_counts['col'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP THESE COLUMNS\n",
    "tr_data.drop(drop_cols, axis=1, inplace=True)\n",
    "if load_unlabled_data: ul_data.drop(drop_cols, axis=1, inplace=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Impute Integer Features\n",
    "\n",
    "For those features that should be of type integer or boolean, impute with most frequent before converting to type integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before imputing all missing values, let's deal with these by themselves\n",
    "# Before converting to integer, we will replace missing values with the mode\n",
    "int_cols=['gcs_eyes_apache', 'gcs_motor_apache', 'gcs_verbal_apache', 'heart_rate_apache', 'map_apache', 'resprate_apache']\n",
    "int_cols=int_cols + ['d1_diasbp_max', 'd1_diasbp_min', 'd1_heartrate_max', 'd1_heartrate_min', 'd1_mbp_max', 'd1_mbp_min', 'd1_resprate_max', 'd1_resprate_min', 'd1_spo2_max', 'd1_spo2_min', 'd1_sysbp_max', 'd1_sysbp_min', 'h1_diasbp_max']\n",
    "int_cols=int_cols + ['h1_diasbp_min', 'h1_heartrate_max', 'h1_heartrate_min', 'h1_mbp_max', 'h1_mbp_min', 'h1_resprate_max', 'h1_resprate_min', 'h1_spo2_max', 'h1_spo2_min', 'h1_sysbp_max', 'h1_sysbp_min', 'd1_glucose_max', 'd1_glucose_min']\n",
    "int_cols=int_cols +['arf_apache', 'gcs_unable_apache', 'intubated_apache', 'ventilated_apache', 'aids', 'cirrhosis', 'diabetes_mellitus']\n",
    "int_cols=int_cols + ['hepatic_failure', 'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis']\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "tr_data[int_cols]  = imputer.fit_transform(tr_data[int_cols])\n",
    "tr_data[int_cols] = tr_data[int_cols].astype(int)\n",
    "\n",
    "if load_unlabled_data: ul_data[int_cols]  = imputer.fit_transform(ul_data[int_cols])\n",
    "if load_unlabled_data: ul_data[int_cols] = ul_data[int_cols].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String/Object Columns\n",
    "\n",
    "Next we will take a look at string/categorical features. Which ones contain missing values or values that can be merged?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_cols=['ethnicity', 'gender', 'hospital_admit_source', 'icu_admit_source',  'icu_stay_type', 'icu_type', 'apache_2_diagnosis', 'apache_3j_diagnosis', 'apache_3j_bodysystem', 'apache_2_bodysystem', 'apache_2_diagnosis', 'apache_3j_diagnosis']\n",
    "tr_data[str_cols] = tr_data[str_cols].astype('str')\n",
    "if load_unlabled_data: ul_data[str_cols] = ul_data[str_cols].astype('str')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up the Data\n",
    "\n",
    "Next we will take a look at string/categorical features. Which ones contain missing values or values that can be merged?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's deal with missing values in our categorical features, like gender, ethnicity, etc.\n",
    "tr_data[\"gender\"].fillna(\"NA\", inplace = True)  \n",
    "tr_data[\"ethnicity\"].fillna(\"Other/Unknown\", inplace = True)  \n",
    "tr_data[\"apache_2_bodysystem\"].fillna(\"Undefined Diagnoses\", inplace = True)  \n",
    "tr_data[\"apache_3j_bodysystem\"].fillna(\"Other\", inplace = True) \n",
    "tr_data[\"icu_admit_source\"].fillna(\"Other\", inplace = True) \n",
    "tr_data[\"hospital_admit_source\"].fillna(\"Other\", inplace = True) \n",
    "\n",
    "# These should really be the same value\n",
    "tr_data.loc[tr_data.loc[:, \"apache_2_bodysystem\"] == \"Undefined diagnoses\", \"apache_2_bodysystem\"] = \"Undefined Diagnoses\"\n",
    "tr_data.loc[tr_data.loc[:, \"apache_2_diagnosis\"] == \"185.40173901455842\", \"apache_2_diagnosis\"] = \"185.0\"\n",
    "\n",
    "\n",
    "if load_unlabled_data: ul_data[\"gender\"].fillna(\"NA\", inplace = True)  \n",
    "if load_unlabled_data: ul_data[\"ethnicity\"].fillna(\"Other/Unknown\", inplace = True)  \n",
    "if load_unlabled_data: ul_data[\"apache_2_bodysystem\"].fillna(\"Undefined Diagnoses\", inplace = True)  \n",
    "if load_unlabled_data: ul_data[\"apache_3j_bodysystem\"].fillna(\"Other\", inplace = True) \n",
    "if load_unlabled_data: ul_data[\"icu_admit_source\"].fillna(\"Other\", inplace = True) \n",
    "if load_unlabled_data: ul_data[\"hospital_admit_source\"].fillna(\"Other\", inplace = True) \n",
    "\n",
    "# These should really be the same value\n",
    "if load_unlabled_data: ul_data.loc[ul_data.loc[:, \"apache_2_bodysystem\"] == \"Undefined diagnoses\", \"apache_2_bodysystem\"] = \"Undefined Diagnoses\"\n",
    "if load_unlabled_data: ul_data.loc[ul_data.loc[:, \"apache_2_diagnosis\"] == \"185.40173901455842\", \"apache_2_diagnosis\"] = \"185.0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "Length of stay in ICU should not be negative, but, before we clean up the data, let's create a new column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tr_data.loc[tr_data.loc[:, \"pre_icu_los_days\"] <0, \"readmitted_to_icu\"] = 1\n",
    "tr_data.loc[tr_data.loc[:, \"pre_icu_los_days\"] >=0, \"readmitted_to_icu\"] = 0\n",
    "tr_data.loc[tr_data.loc[:, \"pre_icu_los_days\"] <0, \"pre_icu_los_days\"] = 0\n",
    "\n",
    "if load_unlabled_data: \n",
    "    ul_data.loc[ul_data.loc[:, \"pre_icu_los_days\"] <0, \"pre_icu_los_days\"] = 0\n",
    "    ul_data.loc[ul_data.loc[:, \"pre_icu_los_days\"] <0, \"readmitted_to_icu\"] = 1\n",
    "    ul_data.loc[ul_data.loc[:, \"pre_icu_los_days\"] >=0, \"readmitted_to_icu\"] = 0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Missing Values for Float Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tr_data.columns:\n",
    "    if tr_data[col].dtypes == 'float64':\n",
    "        if tr_data[col].isna().sum()>0:\n",
    "            HasNan = np.isnan(tr_data[col])\n",
    "            tr_data.loc[HasNan, col] = np.mean(tr_data[col])\n",
    "        if load_unlabled_data:\n",
    "            if ul_data[col].isna().sum()>0:\n",
    "                HasNan = np.isnan(ul_data[col])\n",
    "                ul_data.loc[HasNan, col] = np.mean(ul_data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Outliers for Float Datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_outliers=True\n",
    "if replace_outliers:\n",
    "    for col in tr_data.columns:\n",
    "        if tr_data[col].dtypes in ['float64', 'float16']:\n",
    "            TooHighOrLow =  (tr_data.loc[:, col] >   tr_data[col].mean() + 3 * tr_data[col].std()) | (tr_data.loc[:, col] <   tr_data[col].mean() - 3 * tr_data[col].std())\n",
    "            tr_data.loc[TooHighOrLow, col] = np.median(tr_data.loc[~TooHighOrLow,col])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 28.95 Mb (67.7% reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "int8       29\n",
       "float16    91\n",
       "int16      20\n",
       "object     10\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data = reduce_mem_usage(tr_data)\n",
    "if load_unlabled_data: ul_data = reduce_mem_usage(ul_data)\n",
    "tr_data.dtypes.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91713, 150)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label='hospital_death'\n",
    "X = tr_data.drop([target_label], axis=1)\n",
    "y = tr_data[target_label]\n",
    "\n",
    "if load_unlabled_data: X_ul = ul_data.drop([target_label], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>One Hot Encode the Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objs_num = len(X)\n",
    "if load_unlabled_data: \n",
    "    dataset = pd.concat(objs=[X, X_ul], axis=0)\n",
    "else:\n",
    "    dataset=X\n",
    "    \n",
    "dataset_preprocessed = pd.get_dummies(dataset)\n",
    "columns = dataset_preprocessed.columns\n",
    "\n",
    "if load_unlabled_data:\n",
    "    X = dataset_preprocessed[:train_objs_num]\n",
    "    X_ul = dataset_preprocessed[train_objs_num:]\n",
    "else:\n",
    "    X = dataset_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Scale the Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.scale(X)\n",
    "if load_unlabled_data: X_ul = preprocessing.scale(X_ul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Data Test/Train\n",
    "\n",
    "We will split our newly vectorized data into 80% train and 20% test. Our random state value will ensure that the data is split the same way each time so that we can replicate results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'objective':'binary:logistic', 'n_estimators':100, 'learning_rate':0.01, 'colsample_bytree': 0.8,\n",
    " 'gamma': 0.5,  'max_depth': 3, 'min_child_weight': 3, 'subsample': 0.8}\n",
    "\n",
    "clf = xgb.XGBClassifier(**param_dist)\n",
    "\n",
    "clf.fit(x_train, y_train,\n",
    "        eval_set=[(x_train, y_train), (x_test, y_test)],\n",
    "        eval_metric='logloss',\n",
    "        verbose=False)\n",
    "\n",
    "evals_result = clf.evals_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_evaluate(x_train, y_train, clf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_evaluate(x_test, y_test, clf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Train the Model </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "'''\n",
    "\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "estimator= xgb.XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',  silent=True, nthread=1)\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': [3],\n",
    "    'min_child_weight': [1,3,5],\n",
    "    'learning_rate': [0.1, 0.01, 0.005],\n",
    "    'subsample': [.8, 1.0],\n",
    "    'colsample_bytree': [ .8,1.0],\n",
    "    'gamma': [0.5]\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_grid=parameters,\n",
    "    scoring = 'roc_auc',\n",
    "    n_jobs = 10,\n",
    "    cv = 10,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_evaluate(x_test, y_test, grid_search) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
